Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
[2K-----------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1,684/10,000,000 [0m [ [33m0:00:00[0m < [36m1:23:49[0m , [31m1,988 it/s[0m ]
| time/              |      |
|    fps             | 2046 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
[2K----------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3,720/10,000,000 [0m [ [33m0:00:03[0m < [36m2:48:49[0m , [31m987 it/s[0m ]
| time/                   |            |
|    fps                  | 1082       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.09012115 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | -0.119     |
|    learning_rate        | 0.0003     |
|    loss                 | 124        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0365     |
|    std                  | 1          |
|    value_loss           | 2.22e+04   |
----------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5,882/10,000,000 [0m [ [33m0:00:05[0m < [36m2:29:33[0m , [31m1,114 it/s[0m ]
| time/                   |           |
|    fps                  | 1160      |
|    iterations           | 3         |
|    time_elapsed         | 5         |
|    total_timesteps      | 6144      |
| train/                  |           |
|    approx_kl            | 23.561047 |
|    clip_fraction        | 0.8       |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.27     |
|    explained_variance   | -0.342    |
|    learning_rate        | 0.0003    |
|    loss                 | 493       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.202     |
|    std                  | 1.01      |
|    value_loss           | 4.37e+03  |
---------------------------------------
[2K--------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7,938/10,000,000 [0m [ [33m0:00:06[0m < [36m2:24:43[0m , [31m1,151 it/s[0m ]
| time/                   |          |
|    fps                  | 1186     |
|    iterations           | 4        |
|    time_elapsed         | 6        |
|    total_timesteps      | 8192     |
| train/                  |          |
|    approx_kl            | 16.74848 |
|    clip_fraction        | 0.798    |
|    clip_range           | 0.2      |
|    entropy_loss         | -4.3     |
|    explained_variance   | 0.729    |
|    learning_rate        | 0.0003   |
|    loss                 | 117      |
|    n_updates            | 30       |
|    policy_gradient_loss | 0.189    |
|    std                  | 1.02     |
|    value_loss           | 2.57e+03 |
--------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m10,104/10,000,000 [0m [ [33m0:00:08[0m < [36m2:24:59[0m , [31m1,148 it/s[0m ]
| time/                   |           |
|    fps                  | 1168      |
|    iterations           | 5         |
|    time_elapsed         | 8         |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 36.228905 |
|    clip_fraction        | 0.85      |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.34     |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0003    |
|    loss                 | 106       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.23      |
|    std                  | 1.03      |
|    value_loss           | 1.54e+03  |
---------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11,927/10,000,000 [0m [ [33m0:00:10[0m < [36m2:23:58[0m , [31m1,156 it/s[0m ]
| time/                   |           |
|    fps                  | 1186      |
|    iterations           | 6         |
|    time_elapsed         | 10        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 17.142017 |
|    clip_fraction        | 0.932     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.35     |
|    explained_variance   | 0.242     |
|    learning_rate        | 0.0003    |
|    loss                 | 138       |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.275     |
|    std                  | 1.03      |
|    value_loss           | 819       |
---------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14,003/10,000,000 [0m [ [33m0:00:11[0m < [36m2:22:07[0m , [31m1,171 it/s[0m ]
| time/                   |           |
|    fps                  | 1196      |
|    iterations           | 7         |
|    time_elapsed         | 11        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 35.379257 |
|    clip_fraction        | 0.965     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.36     |
|    explained_variance   | 0.922     |
|    learning_rate        | 0.0003    |
|    loss                 | 348       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.289     |
|    std                  | 1.03      |
|    value_loss           | 1.22e+03  |
---------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m16,066/10,000,000 [0m [ [33m0:00:13[0m < [36m2:18:43[0m , [31m1,200 it/s[0m ]
| time/                   |           |
|    fps                  | 1220      |
|    iterations           | 8         |
|    time_elapsed         | 13        |
|    total_timesteps      | 16384     |
| train/                  |           |
|    approx_kl            | 41.050926 |
|    clip_fraction        | 0.942     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.36     |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0003    |
|    loss                 | 138       |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.285     |
|    std                  | 1.03      |
|    value_loss           | 704       |
---------------------------------------
[2K---------------------------------------━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m18,230/10,000,000 [0m [ [33m0:00:14[0m < [36m2:15:59[0m , [31m1,223 it/s[0m ]
| time/                   |           |
|    fps                  | 1237      |
|    iterations           | 9         |
|    time_elapsed         | 14        |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 37.207764 |
|    clip_fraction        | 0.965     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.36     |
|    explained_variance   | 0.919     |
|    learning_rate        | 0.0003    |
|    loss                 | 169       |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.268     |
|    std                  | 1.03      |
|    value_loss           | 477       |
---------------------------------------
[2K/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified m [ [33m0:00:16[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
[2KTraceback (most recent call last):━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
[2K  File "/root/workspace/college-math/mxm/RL4DA_MXM/enkf_L63/scripts/train-on-the-go.py", line 163, in <module>━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    model.learn(
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    return super().learn(
           ^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 224, in collect_rollouts━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    if not callback.on_step():
           ^^^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    return self._on_step()
           ^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 223, in _on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    continue_training = callback.on_step() and continue_training
                        ^^^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    return self._on_step()
           ^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 223, in _on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    continue_training = callback.on_step() and continue_training
                        ^^^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    return self._on_step()
           ^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 464, in _on_step━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    episode_rewards, episode_lengths = evaluate_policy(
                                       ^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py", line 88, in evaluate_policy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    actions, states = model.predict(
                      ^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/base_class.py", line 557, in predict━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    return self.policy.predict(observation, state, episode_start, deterministic)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/policies.py", line 352, in predict━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    self.set_training_mode(False)
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/stable_baselines3/common/policies.py", line 211, in set_training_mode━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    self.train(mode)
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2846, in train━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    module.train(mode)
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2844, in train━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    self.training = mode
    ^^^^^^^^^^^^^
[2K  File "/root/miniconda3/envs/main/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1944, in __setattr__━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
    params = self.__dict__.get("_parameters")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2KKeyboardInterrupt;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
[35m   0%[0m [38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m19,827/10,000,000 [0m [ [33m0:00:18[0m < [36m2:15:33[0m , [31m1,227 it/s[0m ]
